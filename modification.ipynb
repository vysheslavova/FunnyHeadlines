{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag, sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "from openie import StanfordOpenIE\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import itertools as it\n",
    "import language_check\n",
    "import pickle\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "import gender_guesser.detector as gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "stop_words = stopwords.words('english')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tool = language_check.LanguageTool('en-US')\n",
    "nlp = en_core_web_sm.load()\n",
    "d = gender.Detector()\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From title_dep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idioms = pd.read_csv('idioms_def.csv')\n",
    "articles = pd.read_csv('nyt_150_.csv')\n",
    "with open('nyt_words.pickle', 'rb') as f:\n",
    "    dependencies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nyt_bert_lead_verb = pd.read_csv('final/nyt_bert_lead_verb_.csv')\n",
    "# nyt_bert_lead = pd.read_csv('final/nyt_bert_lead_.csv')\n",
    "# nyt_bert_title_verb = pd.read_csv('final/nyt_bert_title_verb_.csv')\n",
    "# nyt_bert_title = pd.read_csv('final/nyt_bert_title_.csv')\n",
    "# nyt_use_lead_verb = pd.read_csv('final/nyt_use_lead_verb_.csv')\n",
    "# nyt_use_lead = pd.read_csv('final/nyt_use_lead_.csv')\n",
    "# nyt_use_title_verb = pd.read_csv('final/nyt_use_title_verb_.csv')\n",
    "# nyt_use_title = pd.read_csv('final/nyt_use_title_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = pd.read_csv('final/nyt_bert.csv')\n",
    "verb = pd.read_csv('final/nyt_bert_verb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get idioms with examples from https://idioms.thefreedictionary.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d792276232402bad5c1f7ff7360976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "idx = []\n",
    "bad_idx = []\n",
    "with_example = []\n",
    "for i, item in tqdm(verb.iterrows(), total=len(bert)):\n",
    "    example = list(idioms[idioms['idiom'] == item['candidate']]['example'].dropna())\n",
    "    \n",
    "    if len(example) == 0:\n",
    "        name = item['candidate'].lower().replace('-', ' ').replace(' ', '+').replace(\"'\", '%27')\n",
    "        link = f\"https://idioms.thefreedictionary.com/{name}\"\n",
    "        url = requests.get(link).content\n",
    "        soup = BeautifulSoup(url, 'html.parser')\n",
    "        example = []\n",
    "        for ex in soup.find_all('span', 'illustration'):\n",
    "            example.append(ex.text)\n",
    "\n",
    "     \n",
    "    with_example.append({'article': item['article'], 'lead': item['lead'], 'content': item['content'],\n",
    "                         'candidate': item['candidate'], 'pos': item['pos'], 'definition': item['definition'], \n",
    "                         'example': example, 'score': item['score'], 'type': item['type']})\n",
    "with_example = pd.DataFrame(with_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_chunks(title):\n",
    "    \"\"\"\n",
    "    spacy chunks from title\n",
    "    \"\"\"\n",
    "    name = []\n",
    "    doc = nlp(title)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        name.append((chunk.text, chunk.root.head.text, chunk.root.dep_))\n",
    "    return name  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    \"\"\"\n",
    "    get lemma from word\n",
    "    \"\"\"\n",
    "    if pos_tag([word])[0][1].find('V') != -1: \n",
    "        lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    elif word == \"one's\":\n",
    "        lemma = \"one's\"\n",
    "    else: \n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_sentence(word, sentence): \n",
    "    \"\"\"\n",
    "    get lemma with pos\n",
    "    \"\"\"\n",
    "    t = ''\n",
    "    for i, (item, tag) in enumerate(pos_tag(word_tokenize(sentence))):\n",
    "        if item == word:\n",
    "            t = tag\n",
    "    if t.find('V') != -1:\n",
    "        lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    else: \n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_popular(word, sentence): \n",
    "    \"\"\"\n",
    "    check is subject popular\n",
    "    \"\"\"\n",
    "    t = ''\n",
    "    for item in nlp(sentence).ents:\n",
    "        if item.text == word:\n",
    "            t = item.label_\n",
    "    if t == 'PERSON':\n",
    "        return True\n",
    "\n",
    "    adress = 'https://people3.azurewebsites.net/People/Search?SearchString='\n",
    "    url = requests.get(adress + word).content\n",
    "    soup = BeautifulSoup(url, 'html.parser')\n",
    "    if len(soup.find_all(id='item_Name')):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern_verb(idiom, text):\n",
    "    \"\"\"\n",
    "    get pattern for modification where idiom is verb\n",
    "    \"\"\"\n",
    "#     lemmas = [get_lemma(word) for word in word_tokenize(idiom)]\n",
    "    lem = []\n",
    "    wait = False\n",
    "    for item in word_tokenize(idiom): \n",
    "        if wait and item == \"'s\":\n",
    "            lem.append(\"one's\")\n",
    "            wait = False\n",
    "        elif item == 'one':\n",
    "            wait = True\n",
    "        else:\n",
    "            lem.append(item)\n",
    "    lemmas = [get_lemma(word) for word in lem]\n",
    "    predlog = 'IN' in list(map(lambda x: x[1], pos_tag(idiom.split())))\n",
    "    order = {key: i for i, key in enumerate(lemmas)}\n",
    "    dep = ['nsubj', 'dobj']\n",
    "    doc = nlp(' '.join(text))\n",
    "    replace = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "#         print('Chunks:', chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)\n",
    "        dependency = False\n",
    "        for d in dep:\n",
    "            if d in chunk.root.dep_:\n",
    "                dependency = True\n",
    "            if d == 'pobj' and predlog:\n",
    "                dependency = True\n",
    "        if (get_lemma(chunk.text) in lemmas or get_lemma(chunk.root.head.text) in lemmas) and dependency:\n",
    "#             print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "#                 chunk.root.head.text)\n",
    "            num = []\n",
    "            try:\n",
    "                num.append(order[get_lemma(chunk.text)])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                num.append(order[get_lemma(chunk.root.head.text)])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            num = min(num)\n",
    "            replace.append((chunk.text, chunk.root.head.text, chunk.root.dep_, num))\n",
    "    return replace, order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern_adj(idiom, text):\n",
    "    \"\"\"\n",
    "    get pattern for modification where idiom is adj\n",
    "\n",
    "    \"\"\"\n",
    "    lemmas = [get_lemma(word) for word in word_tokenize(idiom)]\n",
    "    predlog = 'IN' in list(map(lambda x: x[1], pos_tag(idiom.split())))\n",
    "    order = {key: i for i, key in enumerate(lemmas)}\n",
    "    dep = ['amod']\n",
    "    doc = nlp(' '.join(text))\n",
    "    replace = []\n",
    "    for token in doc:\n",
    "#         print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "#             [child for child in token.children])\n",
    "#         print('Chunks:', token.text, token.head.text, token.dep_)\n",
    "        token_in = False\n",
    "        for k in lemmas:\n",
    "            if k.find(token.text) != -1:\n",
    "                token_in = True\n",
    "                token_k = k\n",
    "        if token_in and token.dep_ in dep:\n",
    "            \n",
    "#             print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "#                 chunk.root.head.text)\n",
    "            num = []\n",
    "            try:\n",
    "                num.append(order[token_k])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                num.append(order[get_lemma(token.head.text)])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            num = min(num)\n",
    "            replace.append((token.head.text, token_k, token.dep_, num))\n",
    "    return replace, order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pattern(pattern, dependencies, order, idx): \n",
    "    final = {get_lemma(item[1]): [] for item in pattern}\n",
    "    new = {}\n",
    "    for sent in with_example['example'][idx]:\n",
    "        for word in word_tokenize(sent): \n",
    "            if word in final.keys():\n",
    "                k = get_lemma_sentence(word, sent)\n",
    "                try:\n",
    "                    new[k] += final[word]\n",
    "                except KeyError:\n",
    "                    new[k] = final[word]\n",
    "    final = new\n",
    "    final_lems = [get_lemma(item) for item in final.keys()]\n",
    "    not_in = {item: [(item, '', i, 'None', 0)] for item, i in order.items() if item not in final.keys()}\n",
    "    final = {**final, **not_in}\n",
    "    \n",
    "    \n",
    "    subject = False\n",
    "    for subj, obj, dep, score in dependencies: \n",
    "        if re.search('nsubj', dep) or dep == 'ROOT':\n",
    "            subject = True\n",
    "            try:\n",
    "                pat = [item for item in pattern if re.search('nsubj', item[2])]\n",
    "                for p in pat:\n",
    "                    final[get_lemma(p[1])].append((subj, p[1], p[3], dep, score))\n",
    "            except:\n",
    "                pass\n",
    "        elif re.search('obj', dep): \n",
    "            try:\n",
    "                pat = [item for item in pattern if re.search('obj', item[2])]\n",
    "                for p in pat:\n",
    "                    final[get_lemma(p[1])].append((p[1], subj, p[3], dep, score))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    if not subject: \n",
    "        for subj, obj, dep, score in dependencies: \n",
    "            try:\n",
    "                pat = [item for item in pattern if re.search('nsubj', item[2])]\n",
    "                for p in pat:\n",
    "                    final[get_lemma(p[1])].append((p[1], subj, p[3], dep, score))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "            \n",
    "    new_final = {}\n",
    "    for key, value in final.items():\n",
    "        if len(value) == 0:\n",
    "            new_final[key] = [(key, '', order[key], 'None', 0)]\n",
    "        else:\n",
    "            new_final[key] = value\n",
    "\n",
    "    allNames = sorted(final)\n",
    "    combinations = it.product(*(final[Name] for Name in allNames))\n",
    "    final = []\n",
    "    for item in combinations:\n",
    "        sort = sorted(item, key=lambda x: x[2])\n",
    "        words = []\n",
    "        subj_exist = False\n",
    "        subj_name = ''\n",
    "        for x in sort:\n",
    "            if re.search('nsubj', x[3]) or x[3] == 'ROOT':\n",
    "                subj_exist = True\n",
    "        scores = 0\n",
    "        if subj_exist: \n",
    "            subj_name = sort[0][0]\n",
    "        else:\n",
    "            subj_name = sort[0][1]\n",
    "        for w1, w2, _, _, s in sort:\n",
    "            if subj_exist:\n",
    "                words.extend([w1, w2])\n",
    "            else:\n",
    "                words.extend([w2, w1])\n",
    "            scores += s\n",
    "        final_sent = re.sub('\\s+', ' ', ' '.join(words)).rstrip().lstrip()\n",
    "        if final_sent.find(\"one's\") != -1:\n",
    "            if is_popular(subj_name, with_example['article'][idx]):\n",
    "                if d.get_gender(subj_name.split()[0]).find('female') != -1: \n",
    "                    predlog = 'her'\n",
    "                elif d.get_gender(subj_name.split()[0]).find('male') != -1: \n",
    "                    predlog = 'his'\n",
    "            else: \n",
    "                predlog = 'its'\n",
    "            final_sent = final_sent.replace(\"one's\", predlog)\n",
    "        final.append((final_sent, scores))\n",
    "    result = list(filter(lambda x: x[0] != with_example['candidate'][idx], set(final)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(idx):\n",
    "    if with_example['pos'][idx] == 'Verb':\n",
    "        pattern, order = get_pattern_verb(with_example['candidate'][idx], with_example['example'][idx])\n",
    "        return replace_pattern(pattern, dependencies[idx], order, idx)\n",
    "    elif with_example['pos'][idx] == 'Adjective':\n",
    "        pattern, order = get_pattern_adj(with_example['candidate'][idx], with_example['example'][idx])\n",
    "        return replace_pattern_adj(pattern, dependencies[idx], order, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b61d24f0954c3ab06f877da154cdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mods = []\n",
    "for i in tqdm(range(len(with_example)), total=len(with_example)):\n",
    "    mods.append(sorted(list(filter(lambda x: x[1] != 0, run(i))), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inch-perfect': []}\n",
      "Everything Is nsubj\n",
      "I Done nsubj\n",
      "Russia of pobj\n",
      "Inquiry of pobj\n",
      "{'inch-perfect': [('Everything', 'inch-perfect', 0, 'nsubj', 1), ('I', 'inch-perfect', 0, 'nsubj', 0), ('inch-perfect', 'Russia', 0, 'pobj', 3), ('inch-perfect', 'Inquiry', 0, 'pobj', 1)]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I inch-perfects', 0),\n",
       " ('Russia inch-perfect', 3),\n",
       " ('Everythings inch-perfect', 1),\n",
       " ('Inquiry inch-perfect', 1)]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['modification'])[['article', 'lead', 'content', 'candidate', 'pos', 'definition', 'modification']].to_csv('nyt_150_mod_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
