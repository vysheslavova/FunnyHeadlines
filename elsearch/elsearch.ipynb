{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'timeout': 360, 'maxsize': 25}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    es.indices.create(index='myandex')\n",
    "except:\n",
    "    es.indices.delete(index='myandex')\n",
    "    es.indices.create(index='myandex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_index(name_idx):\n",
    "    es.indices.delete(index='myandex')\n",
    "    es.indices.create(index='myandex', body=name_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text, bigrams=False):\n",
    "    if bigrams: \n",
    "        analyzer = {\n",
    "            'analyzer': 'bigram_analyzer'\n",
    "        }\n",
    "    else: \n",
    "        analyzer = {\n",
    "            'analyzer': 'my_analyzer'\n",
    "        }\n",
    "        \n",
    "    body = analyzer\n",
    "    body['text'] = text\n",
    "    tokens = es.indices.analyze(index='myandex', body=body)['tokens']\n",
    "    tokens = [token_info['token'] for token_info in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_es_action(index, doc_id, document):\n",
    "    return {\n",
    "        '_index': index,\n",
    "        '_id': doc_id,\n",
    "        '_source': document\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_actions_generator(path, idx):\n",
    "    with open(path, 'r') as inf:\n",
    "        documents = json.load(inf)\n",
    "        for i, doc in enumerate(documents):   \n",
    "            yield create_es_action('myandex', i + idx, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing_documents(path, idx=0):\n",
    "    for ok, result in parallel_bulk(es, es_actions_generator(path, idx), queue_size=4, thread_count=4, chunk_size=1000):\n",
    "        if not ok:\n",
    "            print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('../compare/articles_main.csv')\n",
    "titles = pd.read_csv('../compare/idioms.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, i, *args):\n",
    "    return pretty_print_result(es.search(index='myandex', body=query, size=3000), i)\n",
    "                        \n",
    "def pretty_print_result(search_result, i):\n",
    "    result = []\n",
    "    res = search_result['hits']\n",
    "    for i, hit in enumerate(res['hits']):\n",
    "        string = hit['_source']['title'] + ' ' + '(' + hit['_source']['type'] + ')'\n",
    "        result.append({'id': hit['_id'], 'score': hit['_score'], 'title': string, 'type': hit['_source']['type']})\n",
    "    return result\n",
    "    \n",
    "                  \n",
    "def get_doc_by_id(doc_id):\n",
    "    return es.get(index='myandex', id=doc_id)['_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_query(fields, text):\n",
    "    query = {\n",
    "        \"query\": {\n",
    "           \"multi_match\" : {\n",
    "               \"fields\" : fields,\n",
    "               \"query\" : text\n",
    "           }\n",
    "       }\n",
    "    }\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_score(lst):\n",
    "    scores = []\n",
    "    for item in lst:\n",
    "        scores.append(item['score'])\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    scores = min_max_scaler.fit_transform(np.array(scores).reshape(-1, 1))[:, 0]\n",
    "    return scores * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_score(lst, scores, popularity):\n",
    "    result = []\n",
    "    for i, item in enumerate(lst):\n",
    "        idx = int(item['id'])\n",
    "        score = scores[i]\n",
    "        title = item['title']\n",
    "        pop = popularity[idx]\n",
    "        final_score = 0.8 * score + 0.2 * pop\n",
    "        result.append(final_score)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filters(lst, filter_len, filter_type, filter_verbs, min_len=3, max_len=10, min_verbs=1):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        lst - result from elascticsearch\n",
    "        filter_len - if True: filter length by min_len and max_len\n",
    "        filter_type - if True: filter by type (book or film)\n",
    "        filter_verbs - if True: filter by min_verbs \n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    for row in lst:\n",
    "        verbs = 0\n",
    "        title = row['title']\n",
    "        string = tokenizer(title, bigrams=False)\n",
    "        text = re.sub(r'[^\\w\\s]', '', title)\n",
    "        words = nltk.tokenize.word_tokenize(text)[:-1]\n",
    "        for word in words: \n",
    "            tag = nltk.pos_tag([word])[0][1]\n",
    "            if re.match('VB*', tag) is not None: \n",
    "                verbs += 1\n",
    "        \n",
    "        if filter_len and filter_type and filter_verbs:\n",
    "            if (max_len + 1 >= len(string) >= min_len + 1) and (verbs >= min_verbs) and (row['type'] == 'film' or row['type'] == 'book'):\n",
    "                result.append(row)\n",
    "        elif filter_len and not filter_type and filter_verbs:\n",
    "            if (max_len + 1 >= len(string) >= min_len + 1) and (verbs >= min_verbs):\n",
    "                result.append(row)\n",
    "        elif filter_len and not filter_type and not filter_verbs:\n",
    "            if (max_len + 1 >= len(string) >= min_len + 1):\n",
    "                result.append(row)\n",
    "        elif filter_len and filter_type and not filter_verbs:\n",
    "            if (max_len + 1 >= len(string) >= min_len + 1) and (row['type'] == 'film' or row['type'] == 'book'):\n",
    "                result.append(row)\n",
    "        if len(result) >= 5:\n",
    "            return result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting(lst):\n",
    "    not_empty = []\n",
    "    empty = []\n",
    "    for i, item in enumerate(lst):\n",
    "        if len(item['result']):\n",
    "            not_empty.append(item)\n",
    "        else:\n",
    "            empty.append(item)\n",
    "\n",
    "    result = sorted(not_empty, key=lambda x: x['result'][0]['score'], reverse=True)\n",
    "    result.extend(empty)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(dataset, path, summary_name):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        dataset - dataset name ('idioms'/'movies'/...)\n",
    "        path - path to dataset\n",
    "        summary_name - name of column with summary/lead/...\n",
    "    output:\n",
    "        pd.DataFrame()\n",
    "    \"\"\"\n",
    "    \n",
    "#     methods = ['unigram', 'bigram', 'bigram_stop']\n",
    "    methods = ['unigram']\n",
    "    all_final = []\n",
    "\n",
    "    print('TOTAL PROGRESS:')\n",
    "    for method in tqdm(methods, total=len(methods)):  \n",
    "        final = []\n",
    "        result = []\n",
    "\n",
    "        print('BEGING METHOD:', method)\n",
    "        with open(f'../index/{dataset}/{method}_idx2.json', 'r') as inf:\n",
    "                index = json.load(inf)\n",
    "        recreate_index(index)\n",
    "\n",
    "        print('Indexing document....')\n",
    "        indexing_documents(path, idx=0)\n",
    "        print('Indexing done')\n",
    "        \n",
    "#         options = [('title', 'title'), ('title', 'all'), ('all', 'title'), ('all', 'all')]\n",
    "        options = [('title', 'title'), ('title', 'all'), ('all', 'title'), ('all', 'all')]\n",
    "#         options = [('title', 'title')]\n",
    "\n",
    "#         if method == 'unigram':\n",
    "#             options.append(('keywords', 'keywords'))\n",
    "        \n",
    "        print('Search begins...') \n",
    "        for j, option in tqdm(enumerate(options), total=len(options)):\n",
    "            query, docs = option\n",
    "            print('Option', j + 1)\n",
    "            result_verbs_all = []\n",
    "            result_verbs = []\n",
    "            result_films = []\n",
    "            result = []\n",
    "            k = 0\n",
    "            for x in range(1000):\n",
    "                top = search(make_query([docs], 'man'), k)\n",
    "                for i, item in tqdm(articles.iterrows(), total=articles.shape[0]):\n",
    "                    if len(top) > 0:\n",
    "                        print('BEGIN')\n",
    "                        k += 1\n",
    "                        title = item['title']\n",
    "                        if query == 'all':\n",
    "                            article = item['title'] + ' ' + item['content']\n",
    "                        else:\n",
    "                            article = item[query]\n",
    "\n",
    "                        if docs == 'all':\n",
    "                            candidate = ['title']\n",
    "                            candidate.extend(summary_name)\n",
    "                        else:\n",
    "                            candidate = [docs]\n",
    "\n",
    "                        top = search(make_query(candidate, article[:7000]), k)\n",
    "\n",
    "                        if len(top):\n",
    "\n",
    "                            # POPULARITY\n",
    "        #                     score = scale_score(top)\n",
    "        #                     rating = weight_score(top, score, titles['score'])\n",
    "        #                     top = [{'id': x['id'], 'score': rating[i], 'title': x['title'], 'type': x['type']} for i, x in enumerate(top)]\n",
    "        #                     top = sorted(top, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "                            top = filters(top, True, False, True)\n",
    "\n",
    "\n",
    "                        result.append({'article_title': title, 'result': top[:5]})\n",
    "\n",
    "        #             result = sorting(result)\n",
    "\n",
    "                        for item in result:\n",
    "                            for i, candidate in enumerate(item['result']):\n",
    "                                final.append({'article': title, 'candidate': candidate['title'], \n",
    "                                             'score': candidate['score'], 'type': 'idiom'})\n",
    "        all_final.append(final)\n",
    "    return pd.DataFrame(all_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = run_all('idioms', '../compare/idioms.json', ['overview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
